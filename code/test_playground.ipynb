{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor's utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0.0, 1e-3)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/Documents/University/Master/Second_year/AML/.venv/lib/python3.9/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[128, 512, 512, 512, 512]\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "# Device configuration\n",
    "#--------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)\n",
    "\n",
    "#--------------------------------\n",
    "# Hyper-parameters\n",
    "#--------------------------------\n",
    "input_size = 3\n",
    "num_classes = 10\n",
    "hidden_size = [128, 512, 512, 512, 512]\n",
    "num_epochs = 20\n",
    "batch_size = 200\n",
    "learning_rate = 2e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg=0.001\n",
    "num_training= 49000\n",
    "num_validation =1000\n",
    "norm_layer = None #norm_layer = 'BN'\n",
    "print(hidden_size)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Load the CIFAR-10 dataset\n",
    "#-------------------------------------------------\n",
    "#################################################################################\n",
    "# TODO: Q3.a Choose the right data augmentation transforms with the right       #\n",
    "# hyper-parameters and put them in the data_aug_transforms variable             #\n",
    "#################################################################################\n",
    "data_aug_transforms = []\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "norm_transform = transforms.Compose(data_aug_transforms+[transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(root='../datasets/',\n",
    "                                           train=True,\n",
    "                                           transform=norm_transform,\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../datasets/',\n",
    "                                          train=False,\n",
    "                                          transform=test_transform\n",
    "                                          )\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Prepare the training and validation splits\n",
    "#-------------------------------------------------\n",
    "mask = list(range(num_training))\n",
    "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Data loader\n",
    "#-------------------------------------------------\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# Convolutional neural network (Q1.a and Q2.a)\n",
    "# Set norm_layer for different networks whether using batch normalization\n",
    "#-------------------------------------------------\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, norm_layer=None):\n",
    "        super(ConvNet, self).__init__()\n",
    "        #################################################################################\n",
    "        # TODO: Initialize the modules required to implement the convolutional layer    #\n",
    "        # described in the exercise.                                                    #\n",
    "        # For Q1.a make use of conv2d and relu layers from the torch.nn module.         #\n",
    "        # For Q2.a make use of BatchNorm2d layer from the torch.nn module.              #\n",
    "        # For Q3.b Use Dropout layer from the torch.nn module.                          #\n",
    "        #################################################################################\n",
    "        layers = []\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # Define the hyperparameters of the Convolutional layers \n",
    "        conv_ker_sz = (3,3) # equivalent to conv_ker_sz = 3\n",
    "        padding = 'same'\n",
    "        conv_stride = 1\n",
    "\n",
    "        # Define the hyperparameters of the MaxPooling layers\n",
    "        max_ker_sz = (2,2) # equivalent to max_ker_sz = 2\n",
    "        max_stride = 2\n",
    "\n",
    "        build_conv_block = lambda in_ch, out_ch: [\n",
    "            nn.Conv2d(in_channels=in_ch, \n",
    "                      out_channels=out_ch, \n",
    "                      kernel_size=conv_ker_sz, padding=padding, stride=conv_stride),\n",
    "            nn.MaxPool2d(kernel_size=max_ker_sz, stride=max_stride),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        prev_size = input_size\n",
    "\n",
    "        for h_size in hidden_layers:\n",
    "            layers += build_conv_block(in_ch=prev_size, out_ch=h_size)\n",
    "            prev_size = h_size\n",
    "\n",
    "        layers += [nn.Flatten(), \n",
    "                   nn.Linear(in_features=prev_size, out_features=num_classes)]\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        # self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # TODO: Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # out = self.layers(x)\n",
    "        out = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# Calculate the model size (Q1.b)\n",
    "# if disp is true, print the model parameters, otherwise, only return the number of parameters.\n",
    "#-------------------------------------------------\n",
    "def PrintModelSize(model, disp=True):\n",
    "    #################################################################################\n",
    "    # TODO: Implement the function to count the number of trainable parameters in   #\n",
    "    # the input model. This useful to track the capacity of the model you are       #\n",
    "    # training                                                                      #\n",
    "    #################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    model_sz = sum( p.numel() for p in model.parameters() if p.requires_grad ) \n",
    "\n",
    "    if disp:\n",
    "        print(model_sz)\n",
    "        return\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return model_sz\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Calculate the model size (Q1.c)\n",
    "# visualize the convolution filters of the first convolution layer of the input model\n",
    "#-------------------------------------------------\n",
    "def VisualizeFilter(model):\n",
    "    #################################################################################\n",
    "    # TODO: Implement the functiont to visualize the weights in the first conv layer#\n",
    "    # in the model. Visualize them as a single image of stacked filters.            #\n",
    "    # You can use matlplotlib.imshow to visualize an image in python                #\n",
    "    #################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # Take the filters of the first convolutional layer\n",
    "    weights = model.layers[0].weight.detach().numpy() if device == 'cpu' else model.layers[0].weight.detach().cpu().numpy()\n",
    "    _, W, H, C = weights.shape\n",
    "\n",
    "    # Define a lambda function to convert the values of the filters into RGB space\n",
    "    to_rgb = lambda X, low, high: (255 * (X - low) / (high - low)).astype(np.int32)\n",
    "\n",
    "    # Define the image grid\n",
    "    nrows, ncols = 8, 16\n",
    "    padding = 1 # padding between one image and the next\n",
    "    grid_shape = (nrows*(W+padding), ncols*(H+padding), C)\n",
    "    img_grid = np.zeros(shape=grid_shape, dtype=np.int32)\n",
    "\n",
    "    for i in range(nrows): # for each row of the grid\n",
    "\n",
    "        for j in range(ncols): # for each column of the grid\n",
    "            \n",
    "            img_idx = i*j + j # compute the image index in [0, 127]\n",
    "\n",
    "            for ch_idx in range(C): # for each channel of the image\n",
    "\n",
    "                # Take the minumum and maximum values of the pixels of an image\n",
    "                min_val = np.min(weights[img_idx, :, :, ch_idx])\n",
    "                max_val = np.max(weights[img_idx, :, :, ch_idx])\n",
    "                \n",
    "                # Copy the pixel RGB values into the image grid\n",
    "                top = i*(H+padding) \n",
    "                left = j*(W+padding)\n",
    "                img_grid[top:top+H, left:left+W, ch_idx] = to_rgb(weights[img_idx, :, :, ch_idx], min_val, max_val)\n",
    "\n",
    "    plt.figure(num=1, figsize=(16, 8))\n",
    "    plt.title('Filters of the first Convolutional Layer')\n",
    "    plt.imshow(img_grid)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): ReLU()\n",
      "    (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (10): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): ReLU()\n",
      "    (15): Flatten(start_dim=1, end_dim=-1)\n",
      "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================\n",
    "# Q1.a: Implementing convolutional neural net in PyTorch\n",
    "#======================================================================================\n",
    "# In this question we will implement a convolutional neural networks using the PyTorch\n",
    "# library.  Please complete the code for the ConvNet class evaluating the model\n",
    "#--------------------------------------------------------------------------------------\n",
    "model = ConvNet(input_size, hidden_size, num_classes, norm_layer=norm_layer).to(device)\n",
    "# Q2.a - Initialize the model with correct batch norm layer\n",
    "\n",
    "model.apply(weights_init)\n",
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7678474\n"
     ]
    }
   ],
   "source": [
    "# Print model size\n",
    "#======================================================================================\n",
    "# Q1.b: Implementing the function to count the number of trainable parameters in the model\n",
    "#======================================================================================\n",
    "PrintModelSize(model, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 16\n",
    "ncols = 8\n",
    "weights = model.layers[0].weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3, 3, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#======================================================================================\n",
    "# Q1.c: Implementing the function to visualize the filters in the first conv layers.\n",
    "# Visualize the filters before training\n",
    "#======================================================================================\n",
    "VisualizeFilter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_accuracy = None\n",
    "accuracy_val = []\n",
    "best_model = type(model)(input_size, hidden_size, num_classes, norm_layer=norm_layer) # get a new instance\n",
    "#best_model = ConvNet(input_size, hidden_size, num_classes, norm_layer=norm_layer)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_iter = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_iter += loss.item()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    loss_train.append(loss_iter/(len(train_loader)*batch_size))\n",
    "\n",
    "    \n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "    \n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_iter += loss.item()\n",
    "        \n",
    "        loss_val.append(loss_iter/(len(val_loader)*batch_size))\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_val.append(accuracy)\n",
    "        print('Validation accuracy is: {} %'.format(accuracy))\n",
    "        #################################################################################\n",
    "        # TODO: Q2.b Implement the early stopping mechanism to save the model which has #\n",
    "        # the model with the best validation accuracy so-far (use best_model).          #\n",
    "        #################################################################################\n",
    "\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        \n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    \n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(loss_train, 'r', label='Train loss')\n",
    "plt.plot(loss_val, 'g', label='Val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(3)\n",
    "plt.plot(accuracy_val, 'r', label='Val accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Q2.b Implement the early stopping mechanism to load the weights from the#\n",
    "# best model so far and perform testing with this model.                        #\n",
    "#################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#Compute accuracy on the test set\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
    "\n",
    "\n",
    "\n",
    "# Q1.c: Implementing the function to visualize the filters in the first conv layers.\n",
    "# Visualize the filters before training\n",
    "VisualizeFilter(model)\n",
    "\n",
    "\n",
    "\n",
    "# Save the model checkpoint\n",
    "#torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d71f58446f5be8e830a7d5d91d25d6c937b8d849070c5142221625ba59d54cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
