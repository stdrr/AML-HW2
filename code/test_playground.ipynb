{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor's utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0.0, 1e-3)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[128, 512, 512, 512, 512]\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "# Device configuration\n",
    "#--------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)\n",
    "\n",
    "#--------------------------------\n",
    "# Hyper-parameters\n",
    "#--------------------------------\n",
    "input_size = 3\n",
    "num_classes = 10\n",
    "hidden_size = [128, 512, 512, 512, 512]\n",
    "num_epochs = 20\n",
    "batch_size = 200\n",
    "learning_rate = 2e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg=0.001\n",
    "num_training= 49000\n",
    "num_validation =1000\n",
    "norm_layer = None #norm_layer = 'BN'\n",
    "print(hidden_size)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Load the CIFAR-10 dataset\n",
    "#-------------------------------------------------\n",
    "#################################################################################\n",
    "# TODO: Q3.a Choose the right data augmentation transforms with the right       #\n",
    "# hyper-parameters and put them in the data_aug_transforms variable             #\n",
    "#################################################################################\n",
    "data_aug_transforms = []\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "norm_transform = transforms.Compose(data_aug_transforms+[transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(root='../datasets/',\n",
    "                                           train=True,\n",
    "                                           transform=norm_transform,\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../datasets/',\n",
    "                                          train=False,\n",
    "                                          transform=test_transform\n",
    "                                          )\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Prepare the training and validation splits\n",
    "#-------------------------------------------------\n",
    "mask = list(range(num_training))\n",
    "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Data loader\n",
    "#-------------------------------------------------\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# Convolutional neural network (Q1.a and Q2.a)\n",
    "# Set norm_layer for different networks whether using batch normalization\n",
    "#-------------------------------------------------\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, norm_layer=None):\n",
    "        super(ConvNet, self).__init__()\n",
    "        #################################################################################\n",
    "        # TODO: Initialize the modules required to implement the convolutional layer    #\n",
    "        # described in the exercise.                                                    #\n",
    "        # For Q1.a make use of conv2d and relu layers from the torch.nn module.         #\n",
    "        # For Q2.a make use of BatchNorm2d layer from the torch.nn module.              #\n",
    "        # For Q3.b Use Dropout layer from the torch.nn module.                          #\n",
    "        #################################################################################\n",
    "        layers = []\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # Define the hyperparameters of the Convolutional layers \n",
    "        conv_ker_sz = (3,3) # equivalent to conv_ker_sz = 3\n",
    "        padding = 'same'\n",
    "        conv_stride = 1\n",
    "\n",
    "        # Define the hyperparameters of the MaxPooling layers\n",
    "        max_ker_sz = (2,2) # equivalent to max_ker_sz = 2\n",
    "        max_stride = 2\n",
    "\n",
    "        build_conv_block = lambda in_ch, out_ch: [\n",
    "            nn.Conv2d(in_channels=in_ch, \n",
    "                      out_channels=out_ch, \n",
    "                      kernel_size=conv_ker_sz, padding=padding, stride=conv_stride),\n",
    "            nn.MaxPool2d(kernel_size=max_ker_sz, stride=max_stride),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "\n",
    "        prev_size = input_size\n",
    "\n",
    "        for h_size in hidden_layers:\n",
    "            layers += build_conv_block(in_ch=prev_size, out_ch=h_size)\n",
    "            prev_size = h_size\n",
    "\n",
    "        layers += [nn.Flatten(), \n",
    "                   nn.Linear(in_features=prev_size, out_features=num_classes)]\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        # self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # TODO: Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # out = self.layers(x)\n",
    "        out = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# Calculate the model size (Q1.b)\n",
    "# if disp is true, print the model parameters, otherwise, only return the number of parameters.\n",
    "#-------------------------------------------------\n",
    "def PrintModelSize(model, disp=True):\n",
    "    #################################################################################\n",
    "    # TODO: Implement the function to count the number of trainable parameters in   #\n",
    "    # the input model. This useful to track the capacity of the model you are       #\n",
    "    # training                                                                      #\n",
    "    #################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    model_sz = sum( p.numel() for p in model.parameters() if p.requires_grad ) \n",
    "\n",
    "    if disp:\n",
    "        print(model_sz)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return model_sz\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Calculate the model size (Q1.c)\n",
    "# visualize the convolution filters of the first convolution layer of the input model\n",
    "#-------------------------------------------------\n",
    "def VisualizeFilter(model):\n",
    "    #################################################################################\n",
    "    # TODO: Implement the functiont to visualize the weights in the first conv layer#\n",
    "    # in the model. Visualize them as a single image of stacked filters.            #\n",
    "    # You can use matlplotlib.imshow to visualize an image in python                #\n",
    "    #################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # Take the filters of the first convolutional layer\n",
    "    weights = model.layers[0].weight.detach().numpy() if device == 'cpu' else model.layers[0].weight.detach().cpu().numpy()\n",
    "    _, W, H, C = weights.shape\n",
    "\n",
    "    # Define a lambda function to convert the values of the filters into RGB space\n",
    "    to_rgb = lambda X, low, high: (255 * (X - low) / (high - low)).astype(np.int32)\n",
    "\n",
    "    # Define the image grid\n",
    "    nrows, ncols = 8, 16\n",
    "    padding = 1 # padding between one image and the next\n",
    "    grid_shape = (nrows*(W+padding), ncols*(H+padding), C)\n",
    "    img_grid = np.zeros(shape=grid_shape, dtype=np.int32)\n",
    "\n",
    "    for i in range(nrows): # for each row of the grid\n",
    "\n",
    "        for j in range(ncols): # for each column of the grid\n",
    "            \n",
    "            img_idx = i*ncols + j # compute the image index in [0, 127]\n",
    "\n",
    "            for ch_idx in range(C): # for each channel of the image\n",
    "\n",
    "                # Take the minumum and maximum values of the pixels of an image\n",
    "                min_val = np.min(weights[img_idx, :, :, ch_idx])\n",
    "                max_val = np.max(weights[img_idx, :, :, ch_idx])\n",
    "                \n",
    "                # Copy the pixel RGB values into the image grid\n",
    "                top = i*(H+padding) \n",
    "                left = j*(W+padding)\n",
    "                img_grid[top:top+H, left:left+W, ch_idx] = to_rgb(weights[img_idx, :, :, ch_idx], min_val, max_val)\n",
    "\n",
    "    plt.figure(num=1, figsize=(16, 8))\n",
    "    plt.title('Filters of the first Convolutional Layer')\n",
    "    plt.imshow(img_grid)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): ReLU()\n",
      "    (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (10): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): ReLU()\n",
      "    (15): Flatten(start_dim=1, end_dim=-1)\n",
      "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================\n",
    "# Q1.a: Implementing convolutional neural net in PyTorch\n",
    "#======================================================================================\n",
    "# In this question we will implement a convolutional neural networks using the PyTorch\n",
    "# library.  Please complete the code for the ConvNet class evaluating the model\n",
    "#--------------------------------------------------------------------------------------\n",
    "model = ConvNet(input_size, hidden_size, num_classes, norm_layer=norm_layer).to(device)\n",
    "# Q2.a - Initialize the model with correct batch norm layer\n",
    "\n",
    "model.apply(weights_init)\n",
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7678474\n"
     ]
    }
   ],
   "source": [
    "# Print model size\n",
    "#======================================================================================\n",
    "# Q1.b: Implementing the function to count the number of trainable parameters in the model\n",
    "#======================================================================================\n",
    "PrintModelSize(model, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 16\n",
    "ncols = 8\n",
    "weights = model.layers[0].weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3, 3, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#======================================================================================\n",
    "# Q1.c: Implementing the function to visualize the filters in the first conv layers.\n",
    "# Visualize the filters before training\n",
    "#======================================================================================\n",
    "VisualizeFilter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7998/1862315309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_accuracy = None\n",
    "accuracy_val = []\n",
    "best_model = type(model)(input_size, hidden_size, num_classes, norm_layer=norm_layer) # get a new instance\n",
    "#best_model = ConvNet(input_size, hidden_size, num_classes, norm_layer=norm_layer)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_iter = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_iter += loss.item()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    loss_train.append(loss_iter/(len(train_loader)*batch_size))\n",
    "\n",
    "    \n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "    \n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_iter += loss.item()\n",
    "        \n",
    "        loss_val.append(loss_iter/(len(val_loader)*batch_size))\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_val.append(accuracy)\n",
    "        print('Validation accuracy is: {} %'.format(accuracy))\n",
    "        #################################################################################\n",
    "        # TODO: Q2.b Implement the early stopping mechanism to save the model which has #\n",
    "        # the model with the best validation accuracy so-far (use best_model).          #\n",
    "        #################################################################################\n",
    "\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        \n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    \n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(loss_train, 'r', label='Train loss')\n",
    "plt.plot(loss_val, 'g', label='Val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(3)\n",
    "plt.plot(accuracy_val, 'r', label='Val accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Q2.b Implement the early stopping mechanism to load the weights from the#\n",
    "# best model so far and perform testing with this model.                        #\n",
    "#################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#Compute accuracy on the test set\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
    "\n",
    "\n",
    "\n",
    "# Q1.c: Implementing the function to visualize the filters in the first conv layers.\n",
    "# Visualize the filters before training\n",
    "VisualizeFilter(model)\n",
    "\n",
    "\n",
    "\n",
    "# Save the model checkpoint\n",
    "#torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d71f58446f5be8e830a7d5d91d25d6c937b8d849070c5142221625ba59d54cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
